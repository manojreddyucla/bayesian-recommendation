{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import concatenate\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True # don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 # limit memory to be allocated\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Given a dataset, first building the Trie to store the frequency count information.\n",
    "'''\n",
    "import pickle\n",
    "import pygtrie\n",
    "\n",
    "trie = pygtrie.StringTrie(separator=',')\n",
    "\n",
    "# Function below increments the count of key in dictionary.\n",
    "def increment_count(song):\n",
    "    if(trie.has_key(song)):\n",
    "        trie[song]+=1\n",
    "    else:\n",
    "        trie[song]=1\n",
    "\n",
    "train_file = open('ml-1m/train.csv', 'r')\n",
    "\n",
    "# Looping through our training dataset\n",
    "for line in train_file:\n",
    "    songs_array = line.strip().split(',')\n",
    "    num_songs = len(songs_array)\n",
    "    \n",
    "    #print(num_songs)\n",
    "    \n",
    "    for i in range(num_songs):\n",
    "        song = songs_array[i]\n",
    "        \n",
    "        increment_count(song)\n",
    "        \n",
    "    for i in range(num_songs-1):\n",
    "        song_1 = songs_array[i] \n",
    "        song_2 = songs_array[i + 1] \n",
    "        song_set = song_1 + ',' + song_2\n",
    "        \n",
    "        increment_count(song_set)\n",
    "            \n",
    "    for i in range(num_songs-2):\n",
    "        song_1 = songs_array[i] \n",
    "        song_2 = songs_array[i + 1] \n",
    "        song_3 = songs_array[i + 2] \n",
    "        song_set = song_1 + ',' + song_2 + ',' + song_3\n",
    "        \n",
    "        increment_count(song_set)\n",
    "        \n",
    "    for i in range(num_songs-3):\n",
    "        song_1 = songs_array[i] \n",
    "        song_2 = songs_array[i + 1] \n",
    "        song_3 = songs_array[i + 2] \n",
    "        song_4 = songs_array[i + 3] \n",
    "        song_set = song_1 + ',' + song_2 + ',' + song_3 + ',' + song_4\n",
    "        \n",
    "        increment_count(song_set)\n",
    "\n",
    "train_file.close()\n",
    "\n",
    "# Saving Trie to disk.\n",
    "pickle.dump(trie, open('ml-1m/ml1m.trie', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading trie from disk.\n",
    "import pickle\n",
    "\n",
    "trie = pickle.load(open( \"ml-1m/ml1m.trie\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_item_pairs = list(trie.items())\n",
    "\n",
    "total_1_gram_count = 0\n",
    "for pair in all_item_pairs:\n",
    "    item_list = pair[0].split(',')\n",
    "    if(len(item_list) == 1):\n",
    "        total_1_gram_count+=pair[1]\n",
    "\n",
    "'''\n",
    "    Given song_prefix (a ordered sequence of items) & candidate_song, get_alpha_beta_values\n",
    "    returns the corresponding alpha & beta values as follows:\n",
    "    alpha: (song_prefix, candidate_song)\n",
    "    beta: (song_prefix, ~candidate_song)\n",
    "'''\n",
    "def get_alpha_beta_values(song_prefix, candidate_song):\n",
    "    \n",
    "    if(song_prefix == ''):\n",
    "        alpha = 0\n",
    "        \n",
    "        list_tuples = list(trie.iteritems(prefix=candidate_song))\n",
    "        for pair in list_tuples:\n",
    "            song_list = pair[0].split(',')\n",
    "            #print(song_list)\n",
    "            if(len(song_list) == 1):\n",
    "                if(song_list[0] == candidate_song):\n",
    "                    alpha = pair[1]    \n",
    "        return (alpha, total_1_gram_count - alpha)\n",
    "    \n",
    "    num_prefix = len(song_prefix.split(','))\n",
    "    \n",
    "    if(trie.has_subtrie(song_prefix) == False):\n",
    "        return (0, 0) # For now\n",
    "    \n",
    "    list_tuples = list(trie.iteritems(prefix=song_prefix))\n",
    "    \n",
    "    # list_tuples will contain the (key, count) pairs\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    for pair in list_tuples:\n",
    "        song_list = pair[0].split(',')\n",
    "        \n",
    "        # song_prefix can be same as the song_list        \n",
    "\n",
    "        if(len(song_list) == (num_prefix + 1)):\n",
    "            if(song_list[num_prefix] == candidate_song):\n",
    "                alpha+=pair[1]\n",
    "            else:\n",
    "                beta+=pair[1]\n",
    "    \n",
    "    return (alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Returns the most popular item after prefix 'three_songs'\n",
    "    (Implements the max approach).\n",
    "'''\n",
    "def get_most_popular(three_songs):\n",
    "    list_tuples = list(trie.iteritems(prefix=three_songs))\n",
    "    freq_song_list = []\n",
    "    for pair in list_tuples:\n",
    "        song_list = pair[0].split(',')\n",
    "        #print(song_list)\n",
    "        if(len(song_list) == 4):\n",
    "            freq_song_list.append((song_list[3], pair[1]))\n",
    "    \n",
    "    freq_song_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return freq_song_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Generating the training file for the Neural Networks.\n",
    "    Generation Process Pseudocode:\n",
    "        Use window approach to attain a context of 4 items observed in a user interaction history.\n",
    "        For each sequence, we generate a positive & negative instance where the observed sequence \n",
    "            is the positive sequence & negative sequence is attained as mentioned in the paper.\n",
    "        Each line of the training file for neural networks consists of [8, 8] statistics.\n",
    "'''\n",
    "input_file = open('ml-1m/train.csv', 'r')\n",
    "\n",
    "output_file = open('ml-1m/nn_train_v4.csv', 'w')\n",
    "\n",
    "count = 0\n",
    "special_count = 0\n",
    "\n",
    "for line in input_file:\n",
    "    \n",
    "    if(count%1000 == 0):\n",
    "        print(count)\n",
    "    \n",
    "    count+=1\n",
    "    \n",
    "    items_array = line.strip().split(',')\n",
    "    array_length = len(items_array)\n",
    "    \n",
    "    if(array_length < 4):\n",
    "        continue\n",
    "        \n",
    "    # Going over the song_array with window of size 4\n",
    "    for i in range(0, array_length-3, 4):\n",
    "        # [i -> i+3] both inclusive\n",
    "        a = items_array[i]\n",
    "        b = items_array[i+1]\n",
    "        c = items_array[i+2]\n",
    "        d = items_array[i+3]\n",
    "        \n",
    "        # Getting the following eight counts:\n",
    "            #d        #(~d)\n",
    "            #cd       #(c~d)\n",
    "            #bcd      #(bc~d)\n",
    "            #abcd     #(abc~d)\n",
    "        (d_alpha, d_beta) = get_alpha_beta_values('', d)\n",
    "        (cd_alpha, cd_beta) = get_alpha_beta_values(c, d)\n",
    "        (bcd_alpha, bcd_beta) = get_alpha_beta_values(b + ',' + c, d)\n",
    "        (abcd_alpha, abcd_beta) = get_alpha_beta_values(a + ',' + b + ',' + c, d)\n",
    "        \n",
    "        # Save the above numbers to a file.\n",
    "        pos_str_to_write = ','.join(map(str, [d_alpha, cd_alpha, bcd_alpha, abcd_alpha, \\\n",
    "                  d_beta, cd_beta, bcd_beta, abcd_beta]))\n",
    "        \n",
    "        \n",
    "        # Negative Example\n",
    "        e = ''\n",
    "        sorted_freq_list = get_most_popular(a + ',' + b + ',' + c)\n",
    "        \n",
    "        if(len(sorted_freq_list) < 2):\n",
    "            # Only 1 element in the list.\n",
    "            # Roll back and find the negative example with b,c,d.\n",
    "            sorted_freq_list = get_most_popular(b + ',' + c)\n",
    "        \n",
    "        if(len(sorted_freq_list) < 2):\n",
    "            sorted_freq_list = get_most_popular(c)\n",
    "        \n",
    "        if(len(sorted_freq_list) < 2):\n",
    "            special_count+=1\n",
    "            continue\n",
    "    \n",
    "        if(sorted_freq_list[0][0] == d):\n",
    "            # Choose the next most popular song\n",
    "            e = sorted_freq_list[1][0]\n",
    "        else:\n",
    "            # Choose the most popular song\n",
    "            e = sorted_freq_list[0][0]\n",
    "\n",
    "        \n",
    "        # Found our negative example: a b c e.\n",
    "        (e_alpha, e_beta) = get_alpha_beta_values('', e)\n",
    "        (ce_alpha, ce_beta) = get_alpha_beta_values(c, e)\n",
    "        (bce_alpha, bce_beta) = get_alpha_beta_values(b + ',' + c, e)\n",
    "        (abce_alpha, abce_beta) = get_alpha_beta_values(a + ',' + b + ',' + c, e)\n",
    "        \n",
    "        # Save the above numbers to a file.\n",
    "        neg_str_to_write = ','.join(map(str, [e_alpha, ce_alpha, bce_alpha, abce_alpha, \\\n",
    "                  e_beta, ce_beta, bce_beta, abce_beta]))\n",
    "\n",
    "        #print('============')\n",
    "        output_file.write( pos_str_to_write + ',' + neg_str_to_write + '\\n')\n",
    "\n",
    "input_file.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the neural networks.\n",
    "\n",
    "from numpy import genfromtxt\n",
    "\n",
    "positive_input = keras.Input(shape=(8,), name=\"positive_inputs\")\n",
    "negative_input = keras.Input(shape=(8,), name=\"negative_inputs\")\n",
    "\n",
    "f_alpha = layers.Dense(1000, input_shape=(8,))\n",
    "f_beta = layers.Dense(1000, input_shape=(8,))\n",
    "\n",
    "temp_alpha = layers.Dense(500, input_shape=(1000,))\n",
    "temp_beta = layers.Dense(500, input_shape=(1000,))\n",
    "\n",
    "alpha_inter_layer = layers.Dense(1, name = \"alpha_intermediate\")\n",
    "beta_inter_layer = layers.Dense(1, name = \"beta_intermediate\")\n",
    "\n",
    "'''\n",
    "    Architecture for the alpha & beta functions:\n",
    "\n",
    "    8 -> 1000 -> 500 -> 1 # alpha\n",
    "    8 -> 1000 -> 500 -> 1 # beta\n",
    "'''\n",
    "\n",
    "f_alpha_pos = alpha_inter_layer(temp_alpha(f_alpha(positive_input)))\n",
    "f_beta_pos = beta_inter_layer(temp_beta(f_beta(positive_input)))\n",
    "\n",
    "f_alpha_neg = alpha_inter_layer(temp_alpha(f_alpha(negative_input)))\n",
    "f_beta_neg = beta_inter_layer(temp_beta(f_beta(negative_input)))\n",
    "\n",
    "pos_num = (f_alpha_pos+1.0)\n",
    "pos_denom = (f_alpha_pos+f_beta_pos+2.0)\n",
    "\n",
    "neg_num = (f_alpha_neg+1.0)\n",
    "neg_denom = (f_alpha_neg+f_beta_neg+2.0)\n",
    "\n",
    "positive_mean = pos_num/pos_denom\n",
    "negative_mean = neg_num/neg_denom\n",
    "\n",
    "pos_negative_difference = keras.layers.Subtract(name=\"final_output\")\\\n",
    "            ([positive_mean, negative_mean])\n",
    "\n",
    "global_model = keras.Model(\n",
    "    inputs=[positive_input, negative_input],\n",
    "    outputs=[pos_negative_difference]\n",
    ")\n",
    "\n",
    "global_model.compile(\n",
    "    optimizer = keras.optimizers.Adagrad(),\n",
    "    loss = tf.keras.losses.Hinge()\n",
    ")\n",
    "\n",
    "'''\n",
    "    Hinge Loss:\n",
    "        max(1-y_true*y_pred, 0)\n",
    "'''\n",
    "\n",
    "train_data = genfromtxt('ml-1m/nn_train.csv', delimiter=',')\n",
    "(num_bcd_instances, _) = train_data.shape\n",
    "\n",
    "train_data = np.log(train_data + 0.1)\n",
    "\n",
    "(pos_data, neg_data) = np.split(train_data, 2, axis = 1)\n",
    "\n",
    "true_output = 1000*np.ones((num_bcd_instances, 1))\n",
    "\n",
    "global_model.fit(\n",
    "    {\"positive_inputs\": pos_data, \"negative_inputs\": neg_data},\n",
    "    {\"final_output\": true_output},\n",
    "    epochs = 20,\n",
    "    batch_size = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Further training can be done below.\n",
    "global_model.fit(\n",
    "    {\"positive_inputs\": pos_data, \"negative_inputs\": neg_data},\n",
    "    {\"final_output\": true_output},\n",
    "    epochs = 20,\n",
    "    batch_size = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_alpha = keras.Model(inputs=global_model.inputs, name=\"neutral_input\",\\\n",
    "                             outputs=global_model.get_layer('alpha_intermediate').output)\n",
    "global_beta = keras.Model(inputs=global_model.inputs, name=\"neutral_input\",\\\n",
    "                             outputs=global_model.get_layer('beta_intermediate').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_model.save('ml-1m/global_ml1m_log_v6.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implements the most popular given context baseline.\n",
    "def most_freq_4gram(song_1, song_2, song_3):\n",
    "    if(trie.has_subtrie(song_1+','+song_2+','+song_3)):\n",
    "        #Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_1+','+song_2+','+song_3))\n",
    "        \n",
    "        # Filter out the length 3 tuples\n",
    "        filtered_list_tuples = []\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>3):\n",
    "                filtered_list_tuples.append(pair)\n",
    "                \n",
    "        (max_key, max_count) = max(filtered_list_tuples, key=lambda item:item[1])\n",
    "        max_key_list = max_key.split(',')\n",
    "        \n",
    "        temp_list = sorted(filtered_list_tuples, key=lambda tup: tup[1], reverse=True)[:20]\n",
    "        return [pair[0].split(',')[3] for pair in temp_list]\n",
    "\n",
    "    \n",
    "    if(trie.has_subtrie(song_2+','+song_3)):\n",
    "        #Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_2+','+song_3))\n",
    "        \n",
    "        # Filter out the length 3 tuples\n",
    "        filtered_list_tuples = []\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>2):\n",
    "                filtered_list_tuples.append(pair)\n",
    "                \n",
    "        (max_key, max_count) = max(filtered_list_tuples, key=lambda item:item[1])\n",
    "        max_key_list = max_key.split(',')\n",
    "            \n",
    "        temp_list = sorted(filtered_list_tuples, key=lambda tup: tup[1], reverse=True)[:20]\n",
    "        return [pair[0].split(',')[2] for pair in temp_list]\n",
    "\n",
    "    if(trie.has_subtrie(song_3)):\n",
    "        #Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_3))\n",
    "        \n",
    "        # Filter out the length 3 tuples\n",
    "        filtered_list_tuples = []\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>1):\n",
    "                filtered_list_tuples.append(pair)\n",
    "                \n",
    "        (max_key, max_count) = max(filtered_list_tuples, key=lambda item:item[1])\n",
    "        max_key_list = max_key.split(',')\n",
    "        \n",
    "        temp_list = sorted(filtered_list_tuples, key=lambda tup: tup[1], reverse=True)[:20]\n",
    "        return [pair[0].split(',')[1] for pair in temp_list]\n",
    "\n",
    "    return sorted_most_popular_song_tuple_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_popular_prefix(songs_prefix):\n",
    "    list_tuples = list(trie.iteritems(prefix=songs_prefix))\n",
    "    prefix_length = len(songs_prefix.split(','))\n",
    "    \n",
    "    interested_pair_list = []\n",
    "    for pair in list_tuples:\n",
    "        song_list = pair[0].split(',')\n",
    "        if(len(song_list) == (prefix_length+1) ):\n",
    "            #Add to the list\n",
    "            interested_pair_list.append((song_list[prefix_length], pair[1]))\n",
    "    \n",
    "    # Now return the most frequent one.\n",
    "    if(len(interested_pair_list) == 0):\n",
    "        return ''\n",
    "    \n",
    "    interested_pair_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return interested_pair_list[0][0]\n",
    "\n",
    "total_1_gram_count = 0\n",
    "\n",
    "most_popular_song_tuple_list = []\n",
    "\n",
    "for pair in all_item_pairs:\n",
    "    song_list = pair[0].split(',')\n",
    "    if(len(song_list) == 1):           \n",
    "        most_popular_song_tuple_list.append((song_list[0], pair[1]))\n",
    "        total_1_gram_count+=pair[1]\n",
    "\n",
    "sorted_most_popular_song_tuple_list = \\\n",
    "    sorted(most_popular_song_tuple_list, key=lambda tup: tup[1], reverse=True)\n",
    "        \n",
    "\n",
    "'''\n",
    "    Function below returns the most popular 4th songs given the first 3 songs.\n",
    "'''\n",
    "def get_most_popular(three_songs):\n",
    "    list_tuples = list(trie.iteritems(prefix=three_songs))\n",
    "    freq_song_list = []\n",
    "    for pair in list_tuples:\n",
    "        song_list = pair[0].split(',')\n",
    "        #print(song_list)\n",
    "        if(len(song_list) == 4):\n",
    "            freq_song_list.append((song_list[3], pair[1]))\n",
    "    \n",
    "    freq_song_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return freq_song_list\n",
    "\n",
    "def get_top_k(list_pairs, k):\n",
    "    # Given a list of pairs returns the top k values\n",
    "    if(k > len(list_pairs)):\n",
    "        k = len(list_pairs)\n",
    "\n",
    "    list_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [pair[0] for pair in list_pairs[:k]]\n",
    "\n",
    "'''\n",
    "    Candidate generation is recall focused.\n",
    "    Given a prefix of items: 'song_1, song_2, song_3', candidates are recommended based on their\n",
    "    appearance with this prefix in the training data.\n",
    "    Higher priority is given to candidates from a longer prefix.\n",
    "'''\n",
    "def generate_candidates(song_1, song_2, song_3):\n",
    "    \n",
    "    candidates = []\n",
    "    threshold_total_candidates = 10\n",
    "    \n",
    "    subset_candidates = []\n",
    "    if(trie.has_subtrie(song_1+','+song_2+','+song_3)):\n",
    "        #Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_1+','+song_2+','+song_3))\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>3):\n",
    "                subset_candidates.append( (pair[0].split(',')[3], pair[1]) )\n",
    "    \n",
    "    if(len(subset_candidates) < threshold_total_candidates):\n",
    "        candidates.extend([item[0] for item in subset_candidates])\n",
    "    else:\n",
    "        return get_top_k(subset_candidates, threshold_total_candidates)\n",
    "        \n",
    "    subset_candidates = []\n",
    "    if(trie.has_subtrie(song_2+','+song_3)):\n",
    "        # Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_2+','+song_3))\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>2):\n",
    "                subset_candidates.append( (pair[0].split(',')[2], pair[1]) )\n",
    "    \n",
    "    if( (len(subset_candidates) + len(candidates))  < threshold_total_candidates):\n",
    "        candidates.extend([item[0] for item in subset_candidates])\n",
    "    else:\n",
    "        candidates.extend(get_top_k(subset_candidates, threshold_total_candidates - len(candidates)))\n",
    "        return candidates\n",
    "        \n",
    "    subset_candidates = []\n",
    "    if(trie.has_subtrie(song_3)):\n",
    "        #Get the one with the max value and return it\n",
    "        list_tuples = list(trie.iteritems(prefix=song_3))\n",
    "        for pair in list_tuples:\n",
    "            if(len(pair[0].split(','))>1):\n",
    "                subset_candidates.append( (pair[0].split(',')[1], pair[1]) )\n",
    "    \n",
    "    if( (len(subset_candidates) + len(candidates))  < threshold_total_candidates):\n",
    "        candidates.extend([item[0] for item in subset_candidates])\n",
    "        return candidates\n",
    "    \n",
    "    candidates.extend(get_top_k(subset_candidates, threshold_total_candidates - len(candidates)))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implements our Bayesian approach.\n",
    "def predict_correct_song(song_1, song_2, song_3, max_song):\n",
    "    \n",
    "    candidates = []\n",
    "\n",
    "    candidates = generate_candidates(song_1, song_2, song_3)\n",
    "    \n",
    "    # Result from max frequency.\n",
    "    if(max_song != \"\"):\n",
    "        candidates.append(max_song)\n",
    "            \n",
    "    #Stores the score for each candidate\n",
    "    to_return_map = {}\n",
    "    \n",
    "    for candidate_song in candidates:\n",
    "        \n",
    "        (x_alpha, x_beta) = get_alpha_beta_values('', candidate_song)\n",
    "        (cx_alpha, cx_beta) = get_alpha_beta_values(song_3, candidate_song)\n",
    "        (bcx_alpha, bcx_beta) = get_alpha_beta_values(song_2 + ',' + song_3, candidate_song)\n",
    "        (abcx_alpha, abcx_beta) = get_alpha_beta_values(song_1 + ',' + song_2 +\\\n",
    "                                                        ',' + song_3, candidate_song)\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_vector = np.log(np.array([x_alpha, cx_alpha, bcx_alpha, abcx_alpha, \\\n",
    "                  x_beta, cx_beta, bcx_beta, abcx_beta]) + 0.1) \n",
    "        \n",
    "        \n",
    "        alpha_value = 0.0\n",
    "        beta_value = 1.0\n",
    "        \n",
    "        method_used = -1\n",
    "        \n",
    "        alpha_value = global_alpha.predict([ [input_vector] , [input_vector] ])[0][0]\n",
    "        beta_value = global_beta.predict([ [input_vector] , [input_vector] ])[0][0]\n",
    "        \n",
    "        final_mean = (alpha_value)/(alpha_value+beta_value)\n",
    "\n",
    "        to_return_map[candidate_song] = final_mean\n",
    "        \n",
    "    \n",
    "    sorted_to_return = [k for k, v in \n",
    "                        sorted(to_return_map.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    return sorted_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the performance on test data.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "test_file = open('ml-1m/test.csv', 'r')\n",
    "test_array = []\n",
    "\n",
    "for line in test_file:\n",
    "    songs_array = line.strip().split(',')\n",
    "    test_array.append(songs_array)\n",
    "\n",
    "bayesian_correct = 0\n",
    "max_correct = 0\n",
    "\n",
    "mrr_sum_bayesian = 0.0\n",
    "mrr_sum_max = 0.0\n",
    "\n",
    "recall_sum_bayesian = 0\n",
    "recall_sum_max = 0\n",
    "\n",
    "for test_playlist in test_array:\n",
    "\n",
    "    if(len(test_playlist) < 23):\n",
    "        continue\n",
    "    \n",
    "    if(total_count%100 == 0):\n",
    "        print(total_count)\n",
    "    \n",
    "    #print('Currently processing: ' + str(total_count))\n",
    "    \n",
    "    song_1 = test_playlist[0]\n",
    "    song_2 = test_playlist[1]\n",
    "    song_3 = test_playlist[2]   \n",
    "    \n",
    "    correct_song = test_playlist[3]\n",
    "    \n",
    "    ground_truth_list = test_playlist[3:23]\n",
    "    \n",
    "    max_rec_list = most_freq_4gram(song_1, song_2, song_3)\n",
    "    \n",
    "    max_song = max_rec_list[0][0]\n",
    "    \n",
    "    # Computing the MRR@20 for Max Approach\n",
    "    # First find rank\n",
    "    current_rank = 0.0\n",
    "    if(correct_song in max_rec_list):\n",
    "        current_rank = 1.0/(max_rec_list.index(correct_song)+1.0)\n",
    "    mrr_sum_max+=current_rank\n",
    "\n",
    "    # Compute recall for max\n",
    "    recall_sum_max += len(set(ground_truth_list).intersection(set(max_rec_list)))\n",
    "    \n",
    "    bayesian_rec_list = predict_correct_song(song_1, song_2, song_3, max_song)\n",
    "    bayesian_rec_list_20 = bayesian_rec_list[:20]\n",
    "    \n",
    "    if(bayesian_rec_list_20[0] == 'i'):\n",
    "        bayesian_rec_list_20 = bayesian_rec_list_20[1:]\n",
    "    \n",
    "    # Computing the MRR@20 for Bayesian\n",
    "    # First find rank\n",
    "    current_rank = 0.0\n",
    "    if(correct_song in bayesian_rec_list_20):\n",
    "        current_rank = 1.0/(bayesian_rec_list_20.index(correct_song)+1.0)\n",
    "    mrr_sum_bayesian+=current_rank\n",
    "    \n",
    "    # Recall for Bayesian\n",
    "    recall_sum_bayesian += len(set(ground_truth_list).intersection(set(bayesian_rec_list_20)))\n",
    "    \n",
    "    bayesian_predicted = bayesian_rec_list_20[0]\n",
    "    \n",
    "    # Computing the accuracy\n",
    "    if(bayesian_predicted == correct_song):\n",
    "        bayesian_correct+=1\n",
    "    \n",
    "    if(max_rec_list[0] == correct_song):\n",
    "        max_correct+=1\n",
    "        \n",
    "    #print('======')    \n",
    "    total_count+=1\n",
    "    \n",
    "print('All Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing results for next-item accuracy\n",
    "print('Bayesian: ' + str(bayesian_correct))\n",
    "print('Max: ' + str(max_correct))\n",
    "print('Total: ' + str(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('MRR Max Approach: ' + str(mrr_sum_max/total_count))\n",
    "print('MRR Bayesian: ' + str(mrr_sum_bayesian/total_count))\n",
    "\n",
    "print('=====================================')\n",
    "\n",
    "print('Recall Max Approach: ' + str(recall_sum_max/(total_count*20)))\n",
    "print('Recall Bayesian: ' + str(recall_sum_bayesian/(total_count*20)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
